{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. Gradient Boosting\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../../utils')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import ds_functions as ds\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "n_samples = 5\n",
    "samples = []\n",
    "\n",
    "for i in range(n_samples):\n",
    "    dataTrain: pd.DataFrame = pd.read_csv('data/prepared_b_s_sample/data%d.csv' %i, sep=';')\n",
    "    dataTest: pd.DataFrame = pd.read_csv('data/prepared_test_sample/data%d.csv' %i, sep=';')\n",
    "        \n",
    "    samples.append({'dataTrain': dataTrain, 'dataTest': dataTest})\n",
    "\n",
    "    dataTest_copy = samples[i]['dataTest'].copy(deep=True)\n",
    "\n",
    "    for feature in dataTest_copy.columns:\n",
    "        if feature not in samples[i]['dataTrain'].columns:\n",
    "            samples[i]['dataTest'] = samples[i]['dataTest'].drop(feature, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import ds_functions as ds\n",
    "\n",
    "labels = [False, True]\n",
    "target = 'exp'\n",
    "\n",
    "n_estimators = [5, 10, 25, 50, 75, 100, 150, 200, 250, 300]\n",
    "max_depths = [5, 10, 25]\n",
    "learning_rate = [.1, .3, .5, .7, .9]\n",
    "\n",
    "cols = len(max_depths)\n",
    "\n",
    "sample_best = [(0, .0, 0)] * n_samples\n",
    "sample_best_gb = [None] * n_samples\n",
    "sample_last_best = [0] * n_samples\n",
    "\n",
    "trnY, trnX, tstY, tstX = [], [], [], []\n",
    "for i in range(n_samples):    \n",
    "    trnY.append(samples[i]['dataTrain'].pop(target).values)\n",
    "    trnX.append(samples[i]['dataTrain'].values)\n",
    "    tstY.append(samples[i]['dataTest'].pop(target).values)\n",
    "    tstX.append(samples[i]['dataTest'].values)\n",
    "\n",
    "# Run algorithm and plot results\n",
    "plt.figure()\n",
    "fig, axs = plt.subplots(1, cols, figsize=(16, 4), squeeze=False)\n",
    "for k in range(cols):\n",
    "    d = max_depths[k]\n",
    "    values = {}\n",
    "    for lr in learning_rate:\n",
    "        values[lr] = []\n",
    "        for _ in range(len(n_estimators)):\n",
    "            values[lr].append(0)\n",
    "    for i in range(n_samples):\n",
    "        for lr in learning_rate:\n",
    "            yvalues = []\n",
    "            for n in n_estimators:\n",
    "                gb = GradientBoostingClassifier(n_estimators=n, max_depth=d, learning_rate=lr)\n",
    "                gb.fit(trnX[i], trnY[i])\n",
    "                prdY = gb.predict(tstX[i])\n",
    "                yvalues.append(metrics.accuracy_score(tstY[i], prdY))\n",
    "                # Check if accuracy is better than best in current sample\n",
    "                if yvalues[-1] > sample_last_best[i]:\n",
    "                    sample_best[i] = (d, lr, n)\n",
    "                    sample_last_best[i] = yvalues[-1]\n",
    "                    sample_best_gb[i] = gb\n",
    "            # Increment total accuracy for current (learning_rate, n_estimators)\n",
    "            for n in range(len(yvalues)):\n",
    "                values[lr][n] += yvalues[n]\n",
    "    # Normalize\n",
    "    for lr in values:\n",
    "        for n in range(len(values[lr])):\n",
    "            values[lr][n] /= n_samples\n",
    "            \n",
    "    ds.multiple_line_chart(n_estimators, values, ax=axs[0, k], title='Gradient Boosting with max_depth = %d' % d,\n",
    "                           xlabel='n_estimators', ylabel='accuracy', percentage=True)\n",
    "plt.show()\n",
    "\n",
    "print('Best results per sample:')\n",
    "for i in range(n_samples):\n",
    "    print('\\tSample %d: depth=%d, learning_rate=%f and n_estimators=%d ==> accuracy=%1.2f'\n",
    "        % (i, *sample_best[i], sample_last_best[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_all = []\n",
    "prd_train_all = []\n",
    "y_test_all = []\n",
    "prd_test_all = []\n",
    "\n",
    "for i in range(n_samples):\n",
    "    y_train_all.extend(trnY[i])\n",
    "    prd_train_all.extend(sample_best_gb[i].predict(trnX[i]))\n",
    "    y_test_all.extend(tstY[i])\n",
    "    prd_test_all.extend(sample_best_gb[i].predict(tstX[i]))\n",
    "    \n",
    "ds.plot_evaluation_results(labels, y_train_all, prd_train_all, y_test_all, prd_test_all, showXTickLabels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(sample_last_best)\n",
    "std = np.std(sample_last_best)\n",
    "confidences = (0.95, 0.99)\n",
    "t_st = (1.812, 2.764)\n",
    "\n",
    "print(f'Mean value: {round(mean, 3)}')\n",
    "\n",
    "for c in range(len(confidences)):\n",
    "    conf = confidences[c]\n",
    "    minAcc = mean - t_st[c] * std / (10**0.5)\n",
    "    maxAcc = mean + t_st[c] * std / (10**0.5)\n",
    "    print(f'{int(conf*100)}% cofidence interval for accuracy: [' +\n",
    "        f'{round(minAcc, 3)},{round(maxAcc, 3)}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = []\n",
    "\n",
    "for k in range(len(max_depths)):\n",
    "    values.append(\n",
    "        {'test': [0 for _ in n_estimators], \n",
    "         'train': [0 for _ in n_estimators]})\n",
    "\n",
    "for i in range(n_samples):\n",
    "    lr = sample_best[i][1]\n",
    "\n",
    "    for k in range(cols):\n",
    "        d = max_depths[k]\n",
    "\n",
    "        yvalues = []\n",
    "        yvalues_train = []\n",
    "        for n in n_estimators:\n",
    "            gb = GradientBoostingClassifier(n_estimators=n, max_depth=d, learning_rate=lr)\n",
    "            gb.fit(trnX[i], trnY[i])\n",
    "            prdY = gb.predict(tstX[i])\n",
    "            prdY_train = gb.predict(trnX[i])\n",
    "            yvalues.append(metrics.accuracy_score(tstY[i], prdY))\n",
    "            yvalues_train.append(metrics.accuracy_score(trnY[i], prdY_train))\n",
    "        values[k]['test'] = [values[k]['test'][v] + \n",
    "            yvalues[v]/n_samples for v in range(len(yvalues))]\n",
    "        values[k]['train'] = [values[k]['train'][v] +\n",
    "            yvalues_train[v]/n_samples for v in range(len(yvalues_train))]\n",
    "\n",
    "plt.figure()\n",
    "fig, axs = plt.subplots(1, len(max_depths), figsize=(16, 4), squeeze=False)\n",
    "for k in range(len(max_depths)):\n",
    "    d = max_depths[k]\n",
    "    ds.multiple_line_chart(n_estimators, values[k], ax=axs[0, k], title='Gradient boosting with depth=%d' % (d),\n",
    "        xlabel='n_estimators', ylabel='accuracy', percentage=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "---\n",
    "\n",
    "***How does performance changes with the number of estimators?***\n",
    "\n",
    "\n",
    "***How does performance changes of learning rate?***\n",
    "\n",
    "\n",
    "***How do models improve with the increase of max_depth?***\n",
    "\n",
    "\n",
    "***What is the best parametrisation (max_depth, number of estimators and learning rate)?***\n",
    "\n",
    "\n",
    "***Is the accuracy achieved good enough?***\n",
    "\n",
    "\n",
    "***What is the largest kind of errors?***\n",
    "\n",
    "\n",
    "***Is it possible to identify overfitting?***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
