{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. KNN\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../../utils')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import ds_functions as ds\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "n_samples = 5\n",
    "samples = []\n",
    "\n",
    "for i in range(n_samples):\n",
    "    dataTrain: pd.DataFrame = pd.read_csv('data/prepared_b_sample/data%d.csv' %i, sep=';')\n",
    "    dataTest: pd.DataFrame = pd.read_csv('data/prepared_test_sample/data%d.csv' %i, sep=';')\n",
    "        \n",
    "    samples.append({'dataTrain': dataTrain, 'dataTest': dataTest})\n",
    "\n",
    "    dataTest_copy = samples[i]['dataTest'].copy(deep=True)\n",
    "\n",
    "    for feature in dataTest_copy.columns:\n",
    "        if feature not in samples[i]['dataTrain'].columns:\n",
    "            samples[i]['dataTest'] = samples[i]['dataTest'].drop(feature, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import ds_functions as ds\n",
    "\n",
    "target = 'exp'\n",
    "\n",
    "my_samples=[]\n",
    "\n",
    "for i in range(n_samples):\n",
    "    trnY = samples[i]['dataTrain'].pop(target).values\n",
    "    trnX = samples[i]['dataTrain'].values\n",
    "    tstY = samples[i]['dataTest'].pop(target).values\n",
    "    tstX = samples[i]['dataTest'].values\n",
    "    \n",
    "    my_samples.append({'X_train': trnX, 'X_test': tstX,\n",
    "                  'y_train': trnY, 'y_test': tstY})\n",
    "    \n",
    "labels = [False, True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import ds_functions as ds\n",
    "\n",
    "sample_best = []\n",
    "last_bests = []\n",
    "\n",
    "for sampl in range(len(my_samples)):\n",
    "    sample = my_samples[sampl]\n",
    "    nvalues = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19]\n",
    "    dist = ['manhattan', 'euclidean', 'chebyshev']\n",
    "    values = {}\n",
    "    best = (0, '')\n",
    "    last_best = 0\n",
    "    for d in dist:\n",
    "        yvalues = []\n",
    "        for n in nvalues:\n",
    "            knn = KNeighborsClassifier(n_neighbors=n, metric=d)\n",
    "            knn.fit(sample['X_train'], sample['y_train'])\n",
    "            prdY = knn.predict(sample['X_test'])\n",
    "            yvalues.append(metrics.accuracy_score(sample['y_test'], prdY))\n",
    "            if yvalues[-1] > last_best:\n",
    "                best = (n, d)\n",
    "                last_best = yvalues[-1]\n",
    "        values[d] = yvalues\n",
    "    sample_best.append(best)\n",
    "    \n",
    "    plt.figure()\n",
    "    ds.multiple_line_chart(nvalues, values, title='KNN variants', xlabel='n', ylabel='accuracy', percentage=True)\n",
    "    plt.show()\n",
    "    print(\"Best accuracy:\", last_best)\n",
    "    last_bests.append(last_best)\n",
    "    print('Best results with %d neighbors and %s'%(best[0], best[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalYTrn = np.array(())\n",
    "totalPrd_trn = np.array(())\n",
    "totalYTst = np.array(())\n",
    "totalPrd_tst = np.array(())\n",
    "\n",
    "for sampl in range(len(my_samples)):\n",
    "    sample = my_samples[sampl]\n",
    "    best = sample_best[sampl]\n",
    "    \n",
    "    clf = KNeighborsClassifier(n_neighbors=best[0], metric=best[1])\n",
    "    clf.fit(sample['X_train'], sample['y_train'])\n",
    "    \n",
    "    prd_trn = clf.predict(sample['X_train'])\n",
    "    totalYTrn = np.concatenate((totalYTrn, sample['y_train']))\n",
    "    totalPrd_trn = np.concatenate((totalPrd_trn, prd_trn))\n",
    "    \n",
    "    prd_tst = clf.predict(sample['X_test'])\n",
    "    totalYTst = np.concatenate((totalYTst, sample['y_test']))\n",
    "    totalPrd_tst = np.concatenate((totalPrd_tst, prd_tst))\n",
    "    \n",
    "ds.plot_evaluation_results(labels, totalYTrn, totalPrd_trn, totalYTst, totalPrd_tst, showXTickLabels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(last_bests)\n",
    "std = np.std(last_bests)\n",
    "confidences = (0.95, 0.99)\n",
    "t_st = (1.812, 2.764)\n",
    "\n",
    "\n",
    "print(f'Mean : {round(mean, 3)}')\n",
    "\n",
    "for c in range(len(confidences)):\n",
    "    conf = confidences[c]\n",
    "    print(f'{int(conf*100)}% cofidence interval for accuracy:')\n",
    "    minAcc = mean - t_st[c] * std / (10**0.5)\n",
    "    maxAcc = mean + t_st[c] * std / (10**0.5)\n",
    "    print(f'[{round(minAcc, 3)},{round(maxAcc, 3)}]')\n",
    "    \n",
    "print(metrics.accuracy_score(totalYTst, totalPrd_tst))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "---\n",
    "\n",
    "***How do models improve with the increase of neighbors?***\n",
    "\n",
    "\n",
    "\n",
    "***How does performance changes with different distance measures?***\n",
    "\n",
    "\n",
    "\n",
    "***What is the best parametrisation (number of neighbors and distance measure)?***\n",
    "\n",
    "\n",
    "\n",
    "***Is the accuracy achieved good enough?***\n",
    "\n",
    "\n",
    "\n",
    "***What is the largest kind of errors?***\n",
    "\n",
    "\n",
    "\n",
    "***Is it possible to identify overfitting?***\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
